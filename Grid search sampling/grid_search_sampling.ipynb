{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized sample importance sampling\n",
    "\n",
    "Because traditional importance sampling seems to reap only marginal results this new approach seeks to identify the dangerous areas within parameter space and increase samples around them.\n",
    "\n",
    "For now lets assume a 2-dimensional parameter space - With a radius 1 circle in the center as its \"danger zone\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isDangerZone(x):\n",
    "    d = np.sqrt( x[0] * x[0] +  x[1] * x[1])\n",
    "    if d <= 1:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The idea\n",
    "#### Searching \n",
    "\n",
    "First we generate samples randomly. This is 1) easier to implement for this proof of concept\n",
    "2) In higher order search spaces grid search will not work anymore.\n",
    "We then evaluate all these samples to find those, which are dangerous. Around these samples, more samples are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSize = 10\n",
    "\n",
    "def generateRandom(size):\n",
    "    return np.random.random() * size - size*0.5\n",
    "\n",
    "def generateSamples(sampleSize):\n",
    "    samples = []\n",
    "    for i in range(0, sampleSize):\n",
    "        samples.append([generateRandom(gridSize), generateRandom(gridSize)])\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDangerousSamples(samples):\n",
    "    return list(filter(isDangerZone,samples))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling\n",
    "\n",
    "Now that the dangerous parameter areas have been found we can continue importance sampling them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSamples(size):\n",
    "    size = 100000\n",
    "    samples = generateSamples(size)\n",
    "    dangerousSamples = getDangerousSamples(samples)\n",
    "    return(len(dangerousSamples)/size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the importance distribution: I am unsure how to do this though there are some interesting papers about it\n",
    "Since the pdf of the function is equal in this example it remains a problem for future me ;)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling around the dangerous samples\n",
    "\n",
    "When sampling around the dangerous samples, an importance distribution is used. In this sense we are importance sampling for every dangerous sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleAroundPoint(sampleSize, point):\n",
    "    samples = []\n",
    "    for i in range(0, int(sampleSize)):\n",
    "\n",
    "        samples.append([point[0] + generateRandom(0.01),point[1] + generateRandom(0.01)])\n",
    "    return samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importanceSampling(size):\n",
    "    inital_frac = 0.1\n",
    "    #importance sampling\n",
    "    #Intial samples\n",
    "    init_samples = inital_frac * size\n",
    "    init_samples = generateSamples(size)\n",
    "    init_dangerous_samples = getDangerousSamples(init_samples)\n",
    "\n",
    "    remainingSamples = size*(1-inital_frac)\n",
    "    dangerousSampleSize = len(init_dangerous_samples)\n",
    "    importanceSamplePerDangerousSample = remainingSamples/dangerousSampleSize\n",
    "\n",
    "    dangerousSamples = 0\n",
    "\n",
    "    for s in init_dangerous_samples:\n",
    "        dangerousSamples += len(getDangerousSamples(sampleAroundPoint(importanceSamplePerDangerousSample, s)))\n",
    "    print(dangerousSamples)\n",
    "    dangerousSamples *= 1/(np.pi)\n",
    "    return dangerousSamples/remainingSamples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8803\n",
      "0.31134243645287885\n",
      "0.03123\n"
     ]
    }
   ],
   "source": [
    "print(importanceSampling(10000))\n",
    "print(randomSamples(10000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
